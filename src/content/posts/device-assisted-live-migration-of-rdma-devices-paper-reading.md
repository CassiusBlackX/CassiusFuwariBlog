---
title: device-assisted-live-migration-of-rdma-devices-paper-reading
published: 2025-10-27
description: 论文 Device-assisted Live Migration of RDMA devices 阅读
image: ''
tags: ["virtualization"]
category: notes
draft: false 
lang: ''
---
# abstract
提出了一种迁移一组直接交互硬件设备的方法，方法对于虚拟机及其网络都透明。提出了一种基于设备的解决方案，包括
+ 通用的设备-虚拟化平台接口
+ 针对NVIDIA connectX系列网络适配器设计并实现的热迁移支持机制
+ 一种新颖的方案，用于静默化通过内存架构（如PCIe）进行的直接通信

# introduction
为了满足AI训练通信需求，RDMA设备可以直接访问访问加速器的内存，从而避免来回在不同的虚拟机内存之间切换。但是为了支持这个场景，hypervisor必须同时暂停每个直通设备，并保持每个设备间通信的一致性。

RDMA技术的特点在于其复杂的软件接口。为了应对这个挑战，早期的RDMA迁移方案采取了协调一致的RDMA资源销毁与重建机制，这在关键路径上引入了额外的开销。在另一些方案，则依赖于RDMA API和通信协议中的非标准扩展。然而，这些方案均需对客户机进行修改，但是云厂商一般不能访问租户的软件栈。

+ section3中 阐述RDMA设备透明迁移至客户机软件及其网络所需遵循的原理及相应的设备支持机制。
  + 扩展该方案，提出一种新颖的技术，用于静默化多个直通设备之间的直接通信
+ section4中展示该技术在PCIe架构中的适用性
  + 与硬件无关
+ section5中，定义一个简单且通用的应用程序编程接口，用于硬件设备的迁移操作，该接口与现有虚拟化基础设施完美兼容。
+ section6中，介绍实现的段都断设备辅助VM迁移方案。
  + 使用connectx系列网络适配器，演示如何静默化复杂设备，并提取其硬件状态，以便在另一台设备上快速重建，同时支持关东更新

# background
## RDMA
+ 硬件网络传输的实现（卸载任务），最小化软件开销，并支持计算与网络的独立推进
+ 操作系统旁路————允许应用程序直接访问硬件，以支持数据路径操作（例如：启动数据传输并检测其完成）
+ 想远程网络对等方暴露DMA，从而实现单边访问

#### IB Verb
InfiniBand的Verb规范，是RDMA API的事实标准，该API由RDMA对象和一组用于操作这些对象的原语组成。RDMA对象的管理（创建或修改）被视为一条控制路径，与数据路径不同，需要操作系统内核的参与。

#### RDMA Objects
在数据路径中，RDMA对象直接从用户空间访问，以最大限度地降低延迟。要启动数据传输，需将工作队列元素(WQE)发布到QueuePair中。传输完成是通过轮询一个CompletionQueue对象中的完成队列条目(CQE)来检测。内存区域(MR)对象用于向本地和远程DMA访问公开应用程序内存。创建MR的对象成为MemoryRegistration，并且是显著的开销所在。

#### RDMA namespace
创建时，RDMA对象会被分配一个整数标识符，这些标识符在于相应对象类型（如QP命名空间）关联的命名空间内是唯一的。这些标识符对客户机操作系统内核和RDMA中间件（即libibverbs）可见。此外，某些标识符，尤其是memory keys(MR Identifiers)和QP编号，还会向远程对等方公开。这些标识符通过应用程序提供的带外通道活经由RDMA连接管理协议建立的通道进行通信。

## 虚拟化系统中的IO
PCIe直通中，客户机软件可直接从虚拟地址空间访问内存映射（MMIO）区域与PCIe设备进行通信，为了使设备能够直接访问虚拟机（如DMA和中断）的同时保持隔离性，引入了IOMMU。为了实现单个PCIe设备在多个虚拟机之间的安全共享，PCIe　SR-IOV扩展允许一个物理功能(PF)创建多个虚拟功能(VF)。后端资源由PF和VF共同共享，同时每个VF都拥有独立的PCIe资源集。在Hypervisor中，VF被视为一个独立的PCIe设备，可安全地直通到目标虚拟机。

## 热迁移
#### pre-copy
在pre-copy阶段，虚拟机状态会在目标系统上逐步重建，同时虚拟机仍继续在源端运行。最初，虚拟机内存快照会被传输到目标端。随后，经过多个预复制轮次，将自上一轮以来被修改（脏页）的内存页面逐步传输过去。为了追踪脏页，hypervisor会利用CPU的MMU。当存在直通设备时，内存还可能因为DMA操作而被修改。此时，虚拟机管理器可借助支持报告IO事务的脏位的现代IOMMU来跟踪这些页面。

#### stop-and-copy
略

#### post-copy
略

#### network-reconfiguration
虚拟机恢复运行后，必须通知网络基础设施其物理位置的变更。在以太网L2网络中，通常通过发送gratuitous ARP reply(gARP)来实现这个目的。而在overlay网络中，则通过SDN控制器执行集中式重构。在IB网络中，是通过重新编程虚拟端口来完成的。

### RDMA迁移
虽然存在软件实现的RDMA方案，但高性能系统仍然依赖于PCI直通技术，将RDMA设备直接暴露给虚拟机。因此，设备状态对虚拟机管理程序而言变得无法访问。目前，对于KVM/QEMU，如果启用了设备直通，会禁用热迁移功能。

#### namespace migration
RDMA技术的特性带来了进一步的挑战。由于RDMA标识符由设备分配，软件无法在目标系统上精确复制RDMA命名空间。为了确保迁移后应用的透明性，基于软件的方法必须在运行时对资源标识符进行翻译，这通常需要对客户机操作系统活中间件进行侵入性修改。

#### in-flight messages
由于传输状态被封装在RDMA设备内部，为确保迁移的一致性，软件解决方案不得不通过完成所有未完成的WQE来耗尽QP。然而，这种方式会显著增加停机时间。另一种方法是探索对QP进行中断终止以实现检查点重启。尽管这种方法适用于双向通信，但会导致原子RDMA操作的状态不一致。此外，这两种方法都容易导致做无用功，因为即使是一个单独终止的WQE,也可能对应GB大小的消息。

#### restore connection
与其它外设不同，RDMA设备的迁移对连接的对等放是可见的。网络地址和RDMA命名空间（如QP编号、memory key）的变更，需要显式的重新创建所有连接到迁移虚拟机的IB QPs，这在现有解决方案中引入了同步依赖性。

为了解决这个问题，基于软件的方法依赖于全局协调。另一种方法是MigrOS，引入了一种新的QP状态——暂停，用于指示远程对等方本虚拟机在迁移中。然而，这种方法容易受到迁移虚拟机故障的影响，甚至可能导致QP被无限期阻塞。

# RDMA设备迁移
RDMA设备复杂度高，持有大量状态信息，并需要在极大规模下提供硬件加速的卓越性能。可以认为，在确保最小化停机时间的同时满足这些期望的RDMA设备起义，必须借助设备本身的辅助功能。以下是一份具有实用价值的迁移方案所应该遵循的原则及其相关的设备辅助功能(DAs)。

## 网络透明
RDMA设备可能拥有数十万个已建立的连接，但同一时刻仅有一小部分处于活跃状态。在这种情况下，与所有远程对等方进行协调不仅成本高昂，而且毫无必要。此外，部分对等放可能也正在迁移。

为了应对这以问题，可对迁移计划进行全局协调。然而，这种方法需要深入了解租户的工作负责——但是云厂商并不知道——而且可能与其它流程（如滚动更新）产生冲突。此外，迁移过程本身也需要一定时间。在pre-copy阶段，已建立连接的集合可能会经历显著的变化，这使得强制保留连接的做法变得不切实际。

因此，无同步的网络透明性是实现可扩展迁移的关键。通过以下原则来实现这一目标：

**DA1**: RDMA命名空间的保持。RDMA命名空间包括链路层可见的QP编号和远程MR的memory key。这些值的任何更改，都需要与远程对等方进行协调。.

在目标设备上精确重建RDMA命名空间，可以消除此类协调要求，并确保迁移的虚拟机完全透明。

**DA2**: 本地连接保持。与RDMA命名空间(DA1)类似，设备必须精确恢复本地网络地址（即MAC/IP或者LID/GID）以及QPs的任何状态。这使得在连接声明周期的任意阶段均可实现迁移，包括连接建立和拆除过程中。

**DA3**: 远程连接的无缝保持。现有方法在迁移前会先断开虚拟机与其远程对等方的连接，待迁移完成后重新建立。而在这个解决方案中，则完全省区了这一步骤。然而，由于挂起的VF无法相应传入数据包，基于可靠连接的协议（TCP和IB RC传输）极易遭遇超时。尽管默认的TCP配置足以应对短暂的宕机时间，但依赖于亚秒级超时的RDMA协议却无法承受迁移操作带来的延迟。为此，采用指数回退机制来解决这一问题。最初，采用极短的超时设置，以弥补正常运行期间偶尔发生的数据包丢失现象；随后，重传之间的延迟将按指数方式逐步延长，从而从容应对迁移过程。

**DA4**: 以数据包粒度进入静默状态。通过耗尽通信来处理当前传输中的消息会导致做无用功，需要同步机制，并引入对网络状况的依赖（如拥塞）

相比之下，设备辅助机制允许数据包为粒度暂停处理，此时重传大小固定且可忽略不计。这种方法需要访问设备的内部状态，包括
+ 当前预期的数据包序列号
+ 传输op code
+ 虚拟RDMA地址
+ 用于重传的原子操作缓存结果

## 在hypervisor中访问设备状态
hypervisor负责保存和恢复设备，确保了对虚拟机内部软件的完全透明。然而，目前尚无标准接口可用于提取VF的状态。正如虚拟机管理程序在不深入分析其内存内容的情况下高效传输虚拟机内存页一样，采用类似的方法，一次性批量获取设备状态，而非逐个遍历RDMA资源

首先，我们将讨论为什么RDMA是“带状态的设备”。
#### architectural state
从IB架构的角度来看，RDMA设备的状态完由已分配对象的集合及其架构状态决定。通过将这一状态序列化并同时捕获对象间的跨依赖关系，转换为另一种更容易读的方式，即可生成一个可移植的镜像，该镜像能够被加载到任何RDMA设备中。

#### 微架构状态
IB架构并未涵盖RDMA网卡的所有方面。首先，它省略了软件接口的定义，例如描述符格式活软硬件间共享的数据结构。其次，RDMA网卡并不局限于IB规范，它可能同时作为以太网网卡使用，并实现一些非标准功能（如加解密、拥塞控制）。为架构状态囊括了网卡的每一个细节，迁移后重新构建状态可确保完整功能得以保留。

**DA5**: 宽松的微架构状态。尽管微架构状态存在不足，但精确捕捉某一特定时刻的为状态既不可行，也缺乏效率。并非所有硬件中的状态都能被读取或恢复，而几乎捕获每个寄存器将导致镜像尺寸巨大。因此，我们主张在状态表示的精确度与内存使用之间取得一个平衡。

**DA6**: 黑箱状态提取。为支持与设备无关的迁移，VF状态必须被视为黑箱处理。此前关于RDMA迁移的研究主要集中在通过IB Verb或其扩展功能实现对象级别的序列化。然而，遍历RDMA对象会因每次访问资源的开销而带来额外的负担，除了性能的影响外，跟踪单个RDMA对象及其依赖关系还会显著增加软件复杂度：每种设备扩展都需要新增迁移逻辑，并相应更新API接口。

**DA7**: 镜像兼容的跟踪。高性能集群通常采用同构硬件构建。然而，为了bug fix或者启用新功能，仍需进行固件更新。为最大限度减少服务停机时间，云提供商通常会实施滚动升级测率，即释放部分服务器以进行维护，并借助热迁移技术实现这一过程。这带来了在不同固件版本的设备之间迁移的挑战。

我们通过引入设备迁移“标签”————一种与特定设备兼容性特征相关的厂商专属比特序列，来应对这一挑战。迁移标签会在源设备和目标设备上分别查询，并根据各厂商的特定规则在软件中进行对比。

## 在虚拟化基础设施中的集成
